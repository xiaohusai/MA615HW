---
title: '615'
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE,echo=FALSE,warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Team members: Jinfei Xue, Guangyan Yu, Yaotang Luo, Shiyu Zhang

# Introduction

We scraped two articles from https://www.correlaid.org/bog. "DATA SCIENCE IS NOT JUST ABOUT DATA SCIENCE", and "ABOUT P-VALUES". Based on these two articles, we analyzed tidy format, frequency, sentiment, relationships between words, and topics.

#
```{r ,echo=FALSE,warning=FALSE,message=FALSE}
#scrape text from blog
library(rvest)
library(tibble)
library(stringr)
library(knitr)

#text1
url1<-"https://correlaid.org/blog/posts/data-science-books-to-read"
url1_web<-read_html(url1)
my_data_1<-html_nodes(url1_web,"div.post-content") %>% html_text
my_data_1<-str_trim(my_data_1,side = "both")#remove the whitespace on both sides
my_data_1<-as.tibble(my_data_1)
colnames(my_data_1) <- "text" 
my_data_1[,"text"]<-gsub("\n","",my_data_1[,"text"]) #remove "\n"

#text2
url2<-"https://correlaid.org/blog/posts/understand-p-values"
url2_web<-read_html(url2)
my_data_2<-html_nodes(url2_web,"div.post-content") %>% html_text
my_data_2<-str_trim(my_data_2,side = "both")#remove the whitespace on both sides
my_data_2<-as.tibble(my_data_2)
colnames(my_data_2) <- "text" 
my_data_2[,"text"]<-gsub("\n","",my_data_2[,"text"]) #remove "\n"
my_data_2[,"text"]<-gsub("p-value","pvalue",my_data_2[,"text"]) #remove "-" in "p-value",or "p" and "value" will be deleted because they are both in stop_words$word
```
#Words frequency 
```{r ,fig.width=10, fig.height=6, fig.cap="Words that occur over 9 times in each text ",echo=FALSE,warning=FALSE,message=FALSE}
library(tidytext)
library(tidyverse)
library(dplyr)
#my_data_1<-data_frame(readLines("lubingxun.txt", encoding="UTF-8"))

#transfer to tidy format
my_data<-list(my_data_1,my_data_2)
book_tidy<-list(NA,NA)
for(i in 1:2){
  book_tidy[i]<-my_data[[i]] %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words)
}

book_tidy[[1]]<-as.tibble(book_tidy[[1]])
book_tidy[[2]]<-as.tibble(book_tidy[[2]])
names(book_tidy[[1]])<-"word"
names(book_tidy[[2]])<-"word"

count<-list(NA,NA)
for(i in 1:2){
  count[[i]]<- book_tidy[[i]] %>% 
    count(word, sort = TRUE)
}
#barplot of times a word occur
library(ggplot2)

p1<-count[[1]] %>%
  filter(n>=9) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(word,n)) +
  geom_col(fill="orange")+
  xlab(NULL) +
  labs(title="text1") +
  coord_flip()
p2<-count[[2]] %>%
  filter(n>=9) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(word,n)) +
  geom_col(fill="pink")+
  xlab(NULL) +
  labs(title="text2") +
  coord_flip()
library(cowplot)
plot<-list(NA,NA)
plot[[1]]<-p1
plot[[2]]<-p2
plot_grid(plotlist = plot)

#frequency<-count %>%
#  mutate(proportion=n/sum(n))

#look in chapters
#chapter_tidy_book <- my_data_1 %>%
  #mutate(linenumber = row_number(),
         #chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
#                                                 ignore_case = TRUE)))) #%>%
#  ungroup() %>%
#  unnest_tokens(word, text)
```

*The barplot shows words that occur over 9 times in each text. We can see that the they both have "data" *

```{r ,echo=FALSE,warning=FALSE,message=FALSE}
#number of words about "trust"
nrc_fear<-get_sentiments("nrc")%>%
  filter(sentiment=="trust")
kable(book_tidy[[1]] %>%
  inner_join(nrc_fear) %>%
  count(word,sort=TRUE),caption="number of words about trust in text1")
kable(book_tidy[[2]] %>%
  inner_join(nrc_fear) %>%
  count(word,sort=TRUE),caption="number of words about trust in text2")
#sentiment score based on "bing", scaled by 50 lines 
library(tidyr)
sentiment_1<- book_tidy[[1]] %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment) %>%
  spread(sentiment,n,fill=0) %>%
  mutate(sentiment=positive-negative)
sentiment_2<-book_tidy[[2]] %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment) %>%
  spread(sentiment,n,fill=0) %>%
  mutate(sentiment=positive-negative)
```


#Sentiment

## Overall sentiment
```{r ,fig.width=8, fig.height=8, fig.cap="sentiment score of each text ",echo=FALSE,warning=FALSE,message=FALSE}
#visualize the sentiment score of each text
library(ggplot2)
text<-c("text1","text2")
sentiment<-c(sentiment_1$sentiment,sentiment_2$sentiment)
s<-as.data.frame(cbind(text,sentiment))
ggplot(s,aes(text,sentiment)) +
  geom_col(show.legend = FALSE,fill="pink") 

```
*The plot shows the sentiment score of two texts, the text1 has higher score so that it has a better sentiment.*


```{r,echo=FALSE,warning=FALSE,message=FALSE}
#3 sentiment dictionaries on this book
afinn_1 <- book_tidy[[1]] %>% 
  inner_join(get_sentiments("afinn")) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")
afinn_2 <- book_tidy[[2]] %>% 
  inner_join(get_sentiments("afinn")) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")
bing_and_nrc_1 <- bind_rows(book_tidy[[1]] %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          book_tidy[[1]] %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  
  count(method, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

bing_and_nrc_2 <- bind_rows(book_tidy[[2]] %>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                          book_tidy[[2]] %>% 
                            inner_join(get_sentiments("nrc") %>% 
                                         filter(sentiment %in% c("positive", 
                                                                 "negative"))) %>%
                            mutate(method = "NRC")) %>%
  
  count(method, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

##Sentiment based on 3 methods

```{r,fig.cap="sentiment based on 3 methods",echo=FALSE,warning=FALSE,message=FALSE}
text<-as.data.frame(c("text1","text1","text1","text2","text2","text2"))
colnames(text)<-"text"
bind_rows(afinn_1, 
          bing_and_nrc_1,afinn_2,bing_and_nrc_2) %>%
  bind_cols(text) %>%
  ggplot(aes(text,sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y") +
  labs(title="sentiment based on 3 methods")
```

*The plot shows that sentiment of text2 based on Bing sentiment dasaset has different result with other two sentiment dataset.*

##Top10 positive and negative words

```{r,echo=FALSE,warning=FALSE,message=FALSE}
#top10 positive and negative words
bing_word_counts_1 <- book_tidy[[1]] %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup() %>%
  top_n(10)

kable(bing_word_counts_1,caption="top 10 positive and negative words for text1")

bing_word_counts_2 <- book_tidy[[2]] %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup() %>%
  top_n(10)

kable(bing_word_counts_2,caption="top 10 positive and negative words for text2")
```

##Words that contribute to positive and negative sentiment

```{r ,fig.width=8, fig.height=4,echo=FALSE,warning=FALSE,message=FALSE}
bing_word_counts_1 %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()

bing_word_counts_2 %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

*The plot shows positive and negative words on both texts.*

#Wordscloud

```{r, fig.height=4, fig.width=4, fig.cap="Words cloud",echo=FALSE,warning=FALSE,message=FALSE}

library(wordcloud)

book_tidy[[1]] %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

book_tidy[[2]] %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```

*The wordscloud shows that "book","science","data","read" are the most frequent in text1; "true","hypothesis","pvalue","model","data","effect"."probability" are the most frequent in text2.*

#Word and document frequency

```{r ,fig.height=5, fig.width=8, fig.cap="Term Frequency Distribution",echo=FALSE,warning=FALSE,message=FALSE}
#term frequency(tf),find words with top 10 tf
#frequency_1
frequency_1<-count[[1]] %>%
  mutate(proportion=n/sum(n))
#frequency_2
frequency_2<-count[[2]] %>%
  mutate(proportion=n/sum(n))
#plot
f1<-frequency_1 %>%
  arrange(desc(proportion)) %>%
  top_n(10) %>%
ggplot(aes(word,proportion)) +
  geom_col(fill="orange")+
  xlab(NULL) +
  coord_flip()
f2<-frequency_2 %>%
  arrange(desc(proportion)) %>%
  top_n(10) %>%
ggplot(aes(word,proportion)) +
  geom_col(fill="orange")+
  xlab(NULL) +
  coord_flip()
#lay ggplot in one row
library(cowplot)
plot<-list(NA,NA)
plot[[1]]<-f1
plot[[2]]<-f2
plot_grid(plotlist = plot)
```

*The plot shows term frequency of top 10 frequent words of each text.*

```{r,echo=FALSE,warning=FALSE,message=FALSE}
#tf_idf
#how important a word is to one text in the two texts.
text1_words<-book_tidy[[1]] %>%
  anti_join(stop_words) %>%
  count(word) %>%
  ungroup()
text1_words<-as.data.frame(cbind(text1_words,rep("text1",dim(text1_words)[1])))
colnames(text1_words)<-c("word","n","text")
text2_words<-book_tidy[[2]] %>%
  anti_join(stop_words) %>%
  count(word) %>%
  ungroup()
text2_words<-as.data.frame(cbind(text2_words,rep("text2",dim(text2_words)[1])))
colnames(text2_words)<-c("word","n","text")
text_words<-bind_rows(text1_words,text2_words)
total1_words<-book_tidy[[1]]%>%
  count(word) %>%
  summarize(total=sum(n))
total1_words<-as.data.frame(cbind(total1_words,"text1"))
colnames(total1_words)<-c("total","text")
total2_words<-book_tidy[[2]]%>%
  count(word) %>%
  summarize(total=sum(n))
total2_words<-as.data.frame(cbind(total2_words,"text2"))
colnames(total2_words)<-c("total","text")
total_words<-bind_rows(total1_words,total2_words)
text_words<-left_join(text_words,total_words)

text_words <-text_words %>%
  bind_tf_idf(word, text, n)
#kable(text_words,caption="tf and tf_idf for 2 texts")

```

```{r fig.height=6,fig.width=8,fig.cap="how important a word is to a document in 2 documents",echo=FALSE,warning=FALSE,message=FALSE}
#tf-idf plot
text_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(text) %>% 
  top_n(10) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = text)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~text, ncol = 2, scales = "free") +
  coord_flip()
```

*The plot shows term frequency based on tf_idf of top 10 frequent words of both text. We can see that the result is almost same with the result of term frequency based on simple term frequency(tf).*

# Relationships between words: n-grams and correlations

## tokenize into consecutive sequences of words(token = "ngrams")

### Counting and filtering n-grams
```{r,echo=FALSE,warning=FALSE,message=FALSE}
library(dplyr)
library(tidytext)

# Text1
data_bigrams <- my_data_1 %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
# examine the most common bigrams 
data_bigrams <- data_bigrams %>%
  count(bigram, sort = TRUE)
# split bigrams(n=2)
bigrams_2 <- my_data_1 %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 2) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE) %>%
  unite(bigram, word1, word2, sep = " ")
head(bigrams_2)
# split bigrams(n=3)
bigrams_3 <- my_data_1 %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE) %>%
  unite(bigram, word1, word2, word3, sep = " ")


# Text2
data2_bigrams <- my_data_2 %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
# examine the most common bigrams 
data2_bigrams <- data_bigrams %>%
  count(bigram, sort = TRUE)
# split bigrams(n=2)
bigrams2_2 <- my_data_2 %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 2) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE) %>%
  unite(bigram, word1, word2, sep = " ")
head(bigrams2_2)
# split bigrams(n=3)
bigrams2_3 <- my_data_2 %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE) %>%
  unite(bigram, word1, word2, word3, sep = " ")

```

*We can see that "data science" and "null hypothesis" are the most common pairs in the two books seperately, which are close to the topic of books.*

###  Analyzing bigrams
```{r,echo=FALSE,warning=FALSE,message=FALSE}
# Text1
bigrams_separated <- data_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

science <- bigrams_filtered %>%
  filter(word2 == "science")
science

# Text 2
bigrams2_separated <- data2_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams2_filtered <- bigrams2_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

science2 <- bigrams2_filtered %>%
  filter(word2 == "science")
science2
```

*We can see that for both two books, the word1(s) are the same corresponding to word2 "science", but with different frequencies.*

### Using bigrams to provide context in sentiment analysis
```{r,fig.height=5, fig.width=8, fig.cap="The words preceded by negative words that had the greatest contribution to sentiment scores, in either a positive or negative direction",echo=FALSE,warning=FALSE,message=FALSE}
# Text 1
# "not"
not <- bigrams_separated %>%
  filter(word1 == "not")%>%
   count(word1, word2, sort = TRUE)

AFINN <- get_sentiments("afinn")

not_words <- bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

library(ggplot2)
not_words %>%
  mutate(contribution = nn * score) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, nn * score, fill = nn * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
# "not", "no", "never", "without"
negation_words <- c("not", "no", "never", "without")

negated_words <- bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  ungroup()

negated_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by negation term") +
  ylab("sentiment score of occurrences") +
  coord_flip()


# Text2
# "not"
not2 <- bigrams2_separated %>%
  filter(word1 == "not")%>%
   count(word1, word2, sort = TRUE)

AFINN <- get_sentiments("afinn")

not2_words <- bigrams2_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

library(ggplot2)
not2_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment score * number of occurrences") +
  coord_flip()
# "not", "no", "never", "without"
negation_words <- c("not", "no", "never", "without")

negated2_words <- bigrams2_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  ungroup()

negated2_words %>%
  mutate(contribution = nn * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, nn * score, fill = nn * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by negation term") +
  ylab("sentiment score of occurrences") +
  coord_flip()



```

*We can see the sentiment scores of occurrences of Words preceded by negation term are the same in the two books. Maybe because the two texts are both downloded from the same blog.*

### Visualizing a network of bigrams with ggraph
```{r,fig.height=5, fig.width=8, fig.cap="Common bigrams in texts, showing those that occurred more than 1 times and where neither word was a stop word",echo=FALSE,warning=FALSE,message=FALSE}
# Text1
library(igraph)
# filter for only relatively common combinations
bigrams_graph <- bigrams_filtered %>%
  filter(n > 1) %>%
  graph_from_data_frame()
# install.packages("ggraph")
library(ggraph)
set.seed(2017)
ggraph(bigrams_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigrams_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()


# Text2
library(igraph)
# filter for only relatively common combinations
bigrams2_graph <- bigrams2_filtered %>%
  head(100)%>%
  graph_from_data_frame()
# install.packages("ggraph")
library(ggraph)
set.seed(2017)
ggraph(bigrams2_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)


```

## Counting and correlating pairs of words with the widyr package

### Counting and correlating among sections
```{r,echo=FALSE,warning=FALSE,message=FALSE}
# Text1
tidy_words <- book_tidy[[1]]%>%
  filter(!word %in% stop_words$word)
# install.packages("widyr")
library(widyr)
# count words co-occuring within sections
tidy_words$section <- rep(1, length(tidy_words))
word_pairs <- tidy_words %>%
  pairwise_count(word, section, sort = TRUE)
data <- word_pairs %>%
  filter(item1 == "data")
data


# Text2
tidy2_words <- book_tidy[[2]]%>%
  filter(!word %in% stop_words$word)
# install.packages("widyr")
library(widyr)
# count words co-occuring within sections
tidy2_words$section <- rep(1, length(tidy2_words))
word2_pairs <- tidy2_words %>%
  pairwise_count(word, section, sort = TRUE)
data2 <- word2_pairs %>%
  filter(item1 == "data")
data2

```

*We can easily find the words that occur with "data" in the two texts*

#5 Latent Dirichlet allocation
```{r,echo=FALSE,warning=FALSE,message=FALSE}
library(topicmodels)
count1 <- count(book_tidy[[1]],word)
count2 <- count(book_tidy[[2]],word)
text1<-rep("text1",dim(count1)[1])
text2<-rep("text2",dim(count2)[1])
a<-as.data.frame(cbind(text1,count1))
b<-as.data.frame(cbind(text2,count2))
colnames(a)<-c("text","word","count")
colnames(b)<-c("text","word","count")
tidy<-as.data.frame(rbind(a,b))

tidy %>%
  cast_dtm(text, word, count)->tidy

#We use the LDA() function from the topicmodels package, setting k = 2, to create a two-topic LDA model
tidylda <- LDA(tidy, k = 2, control = list(seed = 1234))
tidylda

# extracting the per-topic-per-word probabilities.
library(tidytext)
tidytopics <-  tidy(tidylda, matrix = "beta")
tidytopics
```
*The model computes the probability of each term generated by each topic. For example, the term accelerations has 5.598958e-04 probability of being generated from topic 1, but a 7.975985e-04 probability of being generated from topic 2.*

#Word-topic probabilities
```{r,echo=FALSE,warning=FALSE,message=FALSE}
library(ggplot2)
library(dplyr)

tidy_top_words <- tidytopics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

tidy_top_words %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```
*We use top_n function to find the 10 most common terms for each topic. The most common words in topic 1 include "probability", "model", "pvalue" and so on, which shows that it maybe related to statistics. Those most common in topic 2 include "data", "null", and "significant", also suggeting that this topic is related to statistics.*

```{r,echo=FALSE,warning=FALSE,message=FALSE}
library(tidyr)

beta_spread <- tidytopics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread
```
*We can see that the words more common in topic 1 include bayesian, which seems more theoretical. Topic 2 has more word like error, errors, centered, key, interpretation seems much more practical.*

#Document-topic probabilities
```{r,echo=FALSE,warning=FALSE,message=FALSE}
tidy_documents <- tidy(tidylda, matrix = "gamma")

tidy_documents

tidy_documents <-tidy_documents %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE)
tidy_documents
```
*We can see the proportion of words from that document that are generated from that topic. For example, the model estimates that nearly more than half of the words in document 1 were generated from topic 1. Nearly more than half of the words in document 2 were generated from topic 2*
*we how topics are associated with each document.However, these articles does not have chapters so that their values are NA. *

#Per-document classification
```{r,echo=FALSE,warning=FALSE,message=FALSE}
# reorder titles in order of topic 1, topic 2, etc before plotting
tidy_documents %>%
  mutate(title = reorder(title, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title)

chapter_classifications <- tidy_documents %>%
  group_by(title, chapter) %>%
  top_n(1, gamma) %>%
  ungroup()

chapter_classifications

book_topics <- chapter_classifications %>%
  count(title, topic) %>%
  group_by(title) %>%
  top_n(1, n) %>%
  ungroup() %>%
  transmute(consensus = title, topic)


chapter_classifications %>%
  inner_join(book_topics, by = "topic") %>%
  filter(title != consensus)

chapter_classifications
```
*We visualize the gamma probabilities by using box plot.We notice that these two articles were uniquely identified as a single topic each.*

#By word assignments: augment
```{r,echo=FALSE,warning=FALSE,message=FALSE}
assignments <- augment(tidylda, data = tidy)
assignments

assignments <- assignments %>%
  separate(document, c("title", "chapter"), sep = "_", convert = TRUE) %>%
  inner_join(book_topics, by = c(".topic" = "topic"))

assignments
```
*This script returns a tidy data frame for the counts of words. combine this assignments table with the consensus book titles so that we can find incorrect classified words. We can see in the list, the incoorrect classified words have been counted.*

#visualize a confusion matrix
```{r,echo=FALSE,warning=FALSE,message=FALSE}
library(scales)

assignments %>%
  count(title, consensus, wt = count) %>%
  group_by(title) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(consensus, title, fill = percent)) +
  geom_tile() +
  scale_fill_gradient2(high = "red", label = percent_format()) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Book words were assigned to",
       y = "Book words came from",
       fill = "% of assignments")

```
*By visualizing a confusion matrix, we can find how often words from one book were assigned to another. We can find that words are slightly more often assigned to text1.*

#Find out and count mistaken words
```{r,echo=FALSE,warning=FALSE,message=FALSE}
wrong_words <- assignments %>%
  filter(title != consensus)

wrong_words

wrong_words %>%
  count(title, consensus, term, wt = count) %>%
  ungroup() %>%
  arrange(desc(n))
```
*We find the commonly mistaken words, such as null, true, difference and pvalue which are more than 20. *
